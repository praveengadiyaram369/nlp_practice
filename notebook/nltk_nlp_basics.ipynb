{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "prompt-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "empty-magic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
      "\n",
      "The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.\n",
      "\n",
      "The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = 'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.'\n",
    "\n",
    "documents = nltk.sent_tokenize(corpus)\n",
    "for doc in documents:\n",
    "    print(f'{doc}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "furnished-accommodation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.'], ['The', 'result', 'is', 'a', 'computer', 'capable', 'of', '``', 'understanding', \"''\", 'the', 'contents', 'of', 'documents', ',', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', '.'], ['The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves', '.']]\n"
     ]
    }
   ],
   "source": [
    "word_tokens = [nltk.word_tokenize(doc) for doc in documents] \n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fourth-tennis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'NLP', 'subfield', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'language', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data']\n",
      "\n",
      "['The', 'result', 'computer', 'capable', 'understanding', 'contents', 'documents', 'including', 'contextual', 'nuances', 'language', 'within']\n",
      "\n",
      "['The', 'technology', 'accurately', 'extract', 'information', 'insights', 'contained', 'documents', 'well', 'categorize', 'organize', 'documents']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = []\n",
    "regex_str =  '^[\\W_]+$'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for words in word_tokens:\n",
    "    new_doc = [word for word in words if word not in stop_words and not re.match(regex_str, word)]\n",
    "    print(f'{new_doc}\\n')\n",
    "    filtered_tokens.append(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "rational-bachelor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming result\n",
      "['natur', 'languag', 'process', 'nlp', 'subfield', 'linguist', 'comput', 'scienc', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'languag', 'particular', 'program', 'comput', 'process', 'analyz', 'larg', 'amount', 'natur', 'languag', 'data']\n",
      "\n",
      "['the', 'result', 'comput', 'capabl', 'understand', 'content', 'document', 'includ', 'contextu', 'nuanc', 'languag', 'within']\n",
      "\n",
      "['the', 'technolog', 'accur', 'extract', 'inform', 'insight', 'contain', 'document', 'well', 'categor', 'organ', 'document']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "print('Stemming result \\n')\n",
    "\n",
    "for doc in filtered_tokens:\n",
    "    new_doc = [stemmer.stem(word) for word in doc]\n",
    "    print(f'{new_doc}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "young-football",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizer result \n",
      "\n",
      "['Natural', 'language', 'processing', 'NLP', 'subfield', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interaction', 'computer', 'human', 'language', 'particular', 'program', 'computer', 'process', 'analyze', 'large', 'amount', 'natural', 'language', 'data']\n",
      "\n",
      "['The', 'result', 'computer', 'capable', 'understanding', 'content', 'document', 'including', 'contextual', 'nuance', 'language', 'within']\n",
      "\n",
      "['The', 'technology', 'accurately', 'extract', 'information', 'insight', 'contained', 'document', 'well', 'categorize', 'organize', 'document']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print('Lemmatizer result \\n')\n",
    "\n",
    "for doc in filtered_tokens:\n",
    "    new_doc = [lemmatizer.lemmatize(word) for word in doc]\n",
    "    print(f'{new_doc}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
