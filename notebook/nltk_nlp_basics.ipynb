{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "prompt-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "empty-magic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
      "\n",
      "The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.\n",
      "\n",
      "The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paragraph = '''Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.'''\n",
    "\n",
    "documents = nltk.sent_tokenize(paragraph)\n",
    "for doc in documents:\n",
    "    print(f'{doc}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "furnished-accommodation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.'], ['The', 'result', 'is', 'a', 'computer', 'capable', 'of', '``', 'understanding', \"''\", 'the', 'contents', 'of', 'documents', ',', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', '.'], ['The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves', '.']]\n"
     ]
    }
   ],
   "source": [
    "word_tokens = [nltk.word_tokenize(doc) for doc in documents] \n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fourth-tennis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'NLP', 'subfield', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'language', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data']\n",
      "\n",
      "['The', 'result', 'computer', 'capable', 'understanding', 'contents', 'documents', 'including', 'contextual', 'nuances', 'language', 'within']\n",
      "\n",
      "['The', 'technology', 'accurately', 'extract', 'information', 'insights', 'contained', 'documents', 'well', 'categorize', 'organize', 'documents']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = []\n",
    "regex_str =  '^[\\W_]+$'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for words in word_tokens:\n",
    "    new_doc = [word for word in words if word not in stop_words and not re.match(regex_str, word)]\n",
    "    print(f'{new_doc}\\n')\n",
    "    filtered_tokens.append(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rational-bachelor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming result \n",
      "\n",
      "['natur', 'languag', 'process', 'nlp', 'subfield', 'linguist', 'comput', 'scienc', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'languag', 'particular', 'program', 'comput', 'process', 'analyz', 'larg', 'amount', 'natur', 'languag', 'data']\n",
      "\n",
      "['the', 'result', 'comput', 'capabl', 'understand', 'content', 'document', 'includ', 'contextu', 'nuanc', 'languag', 'within']\n",
      "\n",
      "['the', 'technolog', 'accur', 'extract', 'inform', 'insight', 'contain', 'document', 'well', 'categor', 'organ', 'document']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "print('Stemming result \\n')\n",
    "\n",
    "for doc in filtered_tokens:\n",
    "    new_doc = [stemmer.stem(word) for word in doc]\n",
    "    print(f'{new_doc}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "young-football",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizer result \n",
      "\n",
      "['Natural', 'language', 'processing', 'NLP', 'subfield', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interaction', 'computer', 'human', 'language', 'particular', 'program', 'computer', 'process', 'analyze', 'large', 'amount', 'natural', 'language', 'data']\n",
      "\n",
      "['The', 'result', 'computer', 'capable', 'understanding', 'content', 'document', 'including', 'contextual', 'nuance', 'language', 'within']\n",
      "\n",
      "['The', 'technology', 'accurately', 'extract', 'information', 'insight', 'contained', 'document', 'well', 'categorize', 'organize', 'document']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print('Lemmatizer result \\n')\n",
    "\n",
    "for doc in filtered_tokens:\n",
    "    new_doc = [lemmatizer.lemmatize(word) for word in doc]\n",
    "    print(f'{new_doc}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "roman-narrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed data.. \n",
      "\n",
      "natural language processing nlp subfield linguistics computer science artificial intelligence concerned interaction computer human language particular program computer process analyze large amount natural language data \n",
      "\n",
      "result computer capable understanding content document including contextual nuance language within \n",
      "\n",
      "technology accurately extract information insight contained document well categorize organize document \n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "print('transformed data.. \\n')\n",
    "\n",
    "for doc in documents:\n",
    "    data = re.sub('[^a-zA-Z]', ' ', doc)\n",
    "    data = data.lower()\n",
    "    data = data.split()\n",
    "    data = [lemmatizer.lemmatize(word) for word in data if word not in stop_words]\n",
    "    \n",
    "    new_doc = ' '.join(data)\n",
    "    print(f'{new_doc} \\n')\n",
    "    corpus.append(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "private-popularity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 3, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 3, 1,\n",
       "        1, 2, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "leading-average",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accurately', 'amount', 'analyze', 'artificial', 'capable', 'categorize', 'computer', 'concerned', 'contained', 'content', 'contextual', 'data', 'document', 'extract', 'human', 'including', 'information', 'insight', 'intelligence', 'interaction', 'language', 'large', 'linguistics', 'natural', 'nlp', 'nuance', 'organize', 'particular', 'process', 'processing', 'program', 'result', 'science', 'subfield', 'technology', 'understanding', 'well', 'within']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "indie-chapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_ngram = CountVectorizer(analyzer='word', ngram_range=(2, 3))\n",
    "X = cv_ngram.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "stock-wrestling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accurately extract', 'accurately extract information', 'amount natural', 'amount natural language', 'analyze large', 'analyze large amount', 'artificial intelligence', 'artificial intelligence concerned', 'capable understanding', 'capable understanding content', 'categorize organize', 'categorize organize document', 'computer capable', 'computer capable understanding', 'computer human', 'computer human language', 'computer process', 'computer process analyze', 'computer science', 'computer science artificial', 'concerned interaction', 'concerned interaction computer', 'contained document', 'contained document well', 'content document', 'content document including', 'contextual nuance', 'contextual nuance language', 'document including', 'document including contextual', 'document well', 'document well categorize', 'extract information', 'extract information insight', 'human language', 'human language particular', 'including contextual', 'including contextual nuance', 'information insight', 'information insight contained', 'insight contained', 'insight contained document', 'intelligence concerned', 'intelligence concerned interaction', 'interaction computer', 'interaction computer human', 'language data', 'language particular', 'language particular program', 'language processing', 'language processing nlp', 'language within', 'large amount', 'large amount natural', 'linguistics computer', 'linguistics computer science', 'natural language', 'natural language data', 'natural language processing', 'nlp subfield', 'nlp subfield linguistics', 'nuance language', 'nuance language within', 'organize document', 'particular program', 'particular program computer', 'process analyze', 'process analyze large', 'processing nlp', 'processing nlp subfield', 'program computer', 'program computer process', 'result computer', 'result computer capable', 'science artificial', 'science artificial intelligence', 'subfield linguistics', 'subfield linguistics computer', 'technology accurately', 'technology accurately extract', 'understanding content', 'understanding content document', 'well categorize', 'well categorize organize']\n"
     ]
    }
   ],
   "source": [
    "print(cv_ngram.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "streaming-madness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.17842586, 0.17842586, 0.17842586, 0.        ,\n",
       "        0.        , 0.4070924 , 0.17842586, 0.        , 0.        ,\n",
       "        0.        , 0.17842586, 0.        , 0.        , 0.17842586,\n",
       "        0.        , 0.        , 0.        , 0.17842586, 0.17842586,\n",
       "        0.4070924 , 0.17842586, 0.17842586, 0.35685172, 0.17842586,\n",
       "        0.        , 0.        , 0.17842586, 0.17842586, 0.17842586,\n",
       "        0.17842586, 0.        , 0.17842586, 0.17842586, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.32049968,\n",
       "        0.        , 0.24374827, 0.        , 0.        , 0.32049968,\n",
       "        0.32049968, 0.        , 0.24374827, 0.        , 0.        ,\n",
       "        0.32049968, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.24374827, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.32049968, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.32049968, 0.        , 0.        , 0.        ,\n",
       "        0.32049968, 0.        , 0.32049968],\n",
       "       [0.29730323, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.29730323, 0.        , 0.        , 0.29730323, 0.        ,\n",
       "        0.        , 0.        , 0.45221354, 0.29730323, 0.        ,\n",
       "        0.        , 0.29730323, 0.29730323, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.29730323, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.29730323,\n",
       "        0.        , 0.29730323, 0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfvec = TfidfVectorizer()\n",
    "X = tfidfvec.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "southwest-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accurately', 'amount', 'analyze', 'artificial', 'capable', 'categorize', 'computer', 'concerned', 'contained', 'content', 'contextual', 'data', 'document', 'extract', 'human', 'including', 'information', 'insight', 'intelligence', 'interaction', 'language', 'large', 'linguistics', 'natural', 'nlp', 'nuance', 'organize', 'particular', 'process', 'processing', 'program', 'result', 'science', 'subfield', 'technology', 'understanding', 'well', 'within']\n"
     ]
    }
   ],
   "source": [
    "print(tfidfvec.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
