{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cardiovascular-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bacterial-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for category in movie_reviews.categories():\n",
    "    for file_id in movie_reviews.fileids(category):\n",
    "        documents.append((list(movie_reviews.words(file_id)), category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alleged-portfolio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['the', 'happy', 'bastard', \"'\", 's', 'quick', 'movie', 'review', 'damn', 'that', 'y2k', 'bug', '.', 'it', \"'\", 's', 'got', 'a', 'head', 'start', 'in', 'this', 'movie', 'starring', 'jamie', 'lee', 'curtis', 'and', 'another', 'baldwin', 'brother', '(', 'william', 'this', 'time', ')', 'in', 'a', 'story', 'regarding', 'a', 'crew', 'of', 'a', 'tugboat', 'that', 'comes', 'across', 'a', 'deserted', 'russian', 'tech', 'ship', 'that', 'has', 'a', 'strangeness', 'to', 'it', 'when', 'they', 'kick', 'the', 'power', 'back', 'on', '.', 'little', 'do', 'they', 'know', 'the', 'power', 'within', '.', '.', '.', 'going', 'for', 'the', 'gore', 'and', 'bringing', 'on', 'a', 'few', 'action', 'sequences', 'here', 'and', 'there', ',', 'virus', 'still', 'feels', 'very', 'empty', ',', 'like', 'a', 'movie', 'going', 'for', 'all', 'flash', 'and', 'no', 'substance', '.', 'we', 'don', \"'\", 't', 'know', 'why', 'the', 'crew', 'was', 'really', 'out', 'in', 'the', 'middle', 'of', 'nowhere', ',', 'we', 'don', \"'\", 't', 'know', 'the', 'origin', 'of', 'what', 'took', 'over', 'the', 'ship', '(', 'just', 'that', 'a', 'big', 'pink', 'flashy', 'thing', 'hit', 'the', 'mir', ')', ',', 'and', ',', 'of', 'course', ',', 'we', 'don', \"'\", 't', 'know', 'why', 'donald', 'sutherland', 'is', 'stumbling', 'around', 'drunkenly', 'throughout', '.', 'here', ',', 'it', \"'\", 's', 'just', '\"', 'hey', ',', 'let', \"'\", 's', 'chase', 'these', 'people', 'around', 'with', 'some', 'robots', '\"', '.', 'the', 'acting', 'is', 'below', 'average', ',', 'even', 'from', 'the', 'likes', 'of', 'curtis', '.', 'you', \"'\", 're', 'more', 'likely', 'to', 'get', 'a', 'kick', 'out', 'of', 'her', 'work', 'in', 'halloween', 'h20', '.', 'sutherland', 'is', 'wasted', 'and', 'baldwin', ',', 'well', ',', 'he', \"'\", 's', 'acting', 'like', 'a', 'baldwin', ',', 'of', 'course', '.', 'the', 'real', 'star', 'here', 'are', 'stan', 'winston', \"'\", 's', 'robot', 'design', ',', 'some', 'schnazzy', 'cgi', ',', 'and', 'the', 'occasional', 'good', 'gore', 'shot', ',', 'like', 'picking', 'into', 'someone', \"'\", 's', 'brain', '.', 'so', ',', 'if', 'robots', 'and', 'body', 'parts', 'really', 'turn', 'you', 'on', ',', 'here', \"'\", 's', 'your', 'movie', '.', 'otherwise', ',', 'it', \"'\", 's', 'pretty', 'much', 'a', 'sunken', 'ship', 'of', 'a', 'movie', '.'], 'neg')\n"
     ]
    }
   ],
   "source": [
    "print(documents[1])\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exposed-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive']\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for word in movie_reviews.words():\n",
    "    all_words.append(word.lower())\n",
    "    \n",
    "print(all_words[5:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dental-istanbul",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 77717),\n",
       " ('the', 76529),\n",
       " ('.', 65876),\n",
       " ('a', 38106),\n",
       " ('and', 35576),\n",
       " ('of', 34123),\n",
       " ('to', 31937),\n",
       " (\"'\", 30585),\n",
       " ('is', 25195),\n",
       " ('in', 21822)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = nltk.FreqDist(all_words)\n",
    "all_words.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baking-textbook",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words['bastard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "paperback-wesley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hadn', 'why', 'we', \"shan't\", 'wouldn', 'didn', 'these', 'above', 'she', 'all', 'other', 'do', \"hadn't\", 'him', 'because', 'for', 'won', 'same', 'ma', \"doesn't\", 'them', 'then', 'did', \"hasn't\", 'her', 'hers', 'whom', \"that'll\", 'no', 'too', 'about', 'wasn', \"needn't\", \"couldn't\", 'i', 'who', 'should', 'aren', 'further', 'where', 'couldn', \"you'd\", 'just', \"aren't\", 'it', 'was', 's', 't', \"mightn't\", 'again', 'from', \"you'll\", 'they', 'and', 'are', 'mustn', 'very', 'had', \"don't\", \"isn't\", 'a', 'his', 'herself', 'on', \"won't\", \"you're\", \"should've\", 'o', 'been', 'were', 'being', 'while', 'my', 'than', 'doesn', 'to', 'more', 'below', 'some', 'your', 'don', \"didn't\", 'has', 'shouldn', 'its', 'an', 'doing', 'shan', 'hasn', 'during', 'nor', 'in', \"mustn't\", \"it's\", 'before', 'weren', 'isn', 'or', 'over', 'both', 'those', 'once', 'up', 'only', 'as', 'so', 'm', 'what', 'into', \"shouldn't\", 'our', \"wasn't\", 'needn', 'yourself', 'this', 're', 'he', 'is', 'if', 'y', 'can', \"haven't\", 'down', \"you've\", 'which', 'there', 'themselves', \"wouldn't\", 'be', 'but', 'at', 'itself', 'll', 'of', 'each', 'not', 'mightn', 'd', 'the', 'by', 'now', 'have', 'you', 'against', 'through', 'any', 'after', 'such', \"she's\", 'with', 'most', 'yourselves', 'does', 'himself', 'having', \"weren't\", 'their', 'me', 'am', 'own', 'ourselves', 'myself', 'off', 'here', 'when', 'out', 'between', 'how', 'will', 'theirs', 'that', 'ours', 've', 'haven', 'ain', 'under', 'few', 'yours', 'until'}\n"
     ]
    }
   ],
   "source": [
    "stopwords_en = set(stopwords.words('english'))\n",
    "print(stopwords_en)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "annoying-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_doc(doc):\n",
    "    words = doc[0]\n",
    "    filtered_doc = []\n",
    "    \n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word.lower())\n",
    "        if lemma not in stopwords_en and lemma.isalpha():\n",
    "            filtered_doc.append(lemma)\n",
    "    \n",
    "    return ' '.join(filtered_doc), doc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "integrated-poetry",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_X = []\n",
    "labels_y = []\n",
    "\n",
    "for doc in documents:\n",
    "    doc, label = filter_doc(doc)\n",
    "    corpus_X.append(doc)\n",
    "    labels_y.append(label)\n",
    "    \n",
    "assert len(corpus_X) == len(labels_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cheap-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for label in labels_y:\n",
    "    if label == 'neg':\n",
    "        y.append(0)\n",
    "    else:\n",
    "        y.append(1)\n",
    "\n",
    "X_raw = np.array(corpus_X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "sapphire-netherlands",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 69.5%\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=2000)\n",
    "X = tfidf.fit_transform(X_raw).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "model = GaussianNB()\n",
    "preds = model.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test, preds)\n",
    "print(f'Accuracy {accuracy*100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
